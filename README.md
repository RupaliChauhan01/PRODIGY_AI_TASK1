# ğŸ§  Text Generation with GPT-2

This project demonstrates how to generate human-like text using the GPT-2 model from Hugging Face Transformers. It includes fine-tuning GPT-2 on a custom dataset and generating text based on user prompts.

## ğŸ” Project Overview
- Task-01 of the Prodigy Infotech Internship.
- Fine-tunes GPT-2 to generate coherent and contextually relevant text.
- Uses a simple Python pipeline with Hugging Face and PyTorch.

## ğŸš€ Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- VS Code / Google Colab

## ğŸ“ Files
- `gpt2_finetune.py` â€“ For training the model on custom data.
- `generate_text.py` â€“ For generating text using the fine-tuned model.
- `custom_data.txt` â€“ Your dataset file.
- `requirements.txt` â€“ Required Python libraries.

## ğŸ“Œ How to Run
1. Clone the repo  
2. Install dependencies# PRODIGY_AI_TASK1
This project focuses on building a text generation model using GPT-2, a transformer-based language model developed by OpenAl. The goal is to generate coherent and contextually relevant text based on a given prompt by fine-tuning GPT-2 on a custom dataset.  
