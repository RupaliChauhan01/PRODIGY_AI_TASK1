# 🧠 Text Generation with GPT-2

This project demonstrates how to generate human-like text using the GPT-2 model from Hugging Face Transformers. It includes fine-tuning GPT-2 on a custom dataset and generating text based on user prompts.

## 🔍 Project Overview
- Task-01 of the Prodigy Infotech Internship.
- Fine-tunes GPT-2 to generate coherent and contextually relevant text.
- Uses a simple Python pipeline with Hugging Face and PyTorch.

## 🚀 Technologies Used
- Python
- PyTorch
- Hugging Face Transformers
- VS Code / Google Colab

## 📁 Files
- `gpt2_finetune.py` – For training the model on custom data.
- `generate_text.py` – For generating text using the fine-tuned model.
- `custom_data.txt` – Your dataset file.
- `requirements.txt` – Required Python libraries.

## 📌 How to Run
1. Clone the repo  
2. Install dependencies# PRODIGY_AI_TASK1
This project focuses on building a text generation model using GPT-2, a transformer-based language model developed by OpenAl. The goal is to generate coherent and contextually relevant text based on a given prompt by fine-tuning GPT-2 on a custom dataset.  
